{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li7pEMbNfi2o",
        "outputId": "93f2591b-e9f5-43a1-fe77-6dd6004f0625"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers Data:\n",
            "  CustomerID        CustomerName         Region  SignupDate\n",
            "0      C0001    Lawrence Carroll  South America  2022-07-10\n",
            "1      C0002      Elizabeth Lutz           Asia  2022-02-13\n",
            "2      C0003      Michael Rivera  South America  2024-03-07\n",
            "3      C0004  Kathleen Rodriguez  South America  2022-10-09\n",
            "4      C0005         Laura Weber           Asia  2022-08-15\n",
            "Products Data:\n",
            "  ProductID              ProductName     Category   Price\n",
            "0      P001     ActiveWear Biography        Books  169.30\n",
            "1      P002    ActiveWear Smartwatch  Electronics  346.30\n",
            "2      P003  ComfortLiving Biography        Books   44.12\n",
            "3      P004            BookWorld Rug   Home Decor   95.69\n",
            "4      P005          TechPro T-Shirt     Clothing  429.31\n",
            "Transactions Data:\n",
            "  TransactionID CustomerID ProductID      TransactionDate  Quantity  \\\n",
            "0        T00001      C0199      P067  2024-08-25 12:38:23         1   \n",
            "1        T00112      C0146      P067  2024-05-27 22:23:54         1   \n",
            "2        T00166      C0127      P067  2024-04-25 07:38:55         1   \n",
            "3        T00272      C0087      P067  2024-03-26 22:55:37         2   \n",
            "4        T00363      C0070      P067  2024-03-21 15:10:10         3   \n",
            "\n",
            "   TotalValue   Price  \n",
            "0      300.68  300.68  \n",
            "1      300.68  300.68  \n",
            "2      300.68  300.68  \n",
            "3      601.36  300.68  \n",
            "4      902.04  300.68  \n"
          ]
        }
      ],
      "source": [
        "#Step 1: Load and Explore the Data\n",
        "import pandas as pd\n",
        "\n",
        "# Load datasets\n",
        "customers_df = pd.read_csv('/content/Customers.csv')\n",
        "products_df = pd.read_csv('/content/Products.csv')\n",
        "transactions_df = pd.read_csv('/content/Transactions.csv')\n",
        "\n",
        "# Preview the datasets\n",
        "print(\"Customers Data:\")\n",
        "print(customers_df.head())\n",
        "\n",
        "print(\"Products Data:\")\n",
        "print(products_df.head())\n",
        "\n",
        "print(\"Transactions Data:\")\n",
        "print(transactions_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 2: Data Preprocessing and Cleaning\n",
        "# Convert date columns to datetime format\n",
        "customers_df['SignupDate'] = pd.to_datetime(customers_df['SignupDate'])\n",
        "transactions_df['TransactionDate'] = pd.to_datetime(transactions_df['TransactionDate'])\n",
        "\n",
        "# Merge datasets to create a comprehensive customer profile\n",
        "merged_df = transactions_df.merge(customers_df, on='CustomerID').merge(products_df, on='ProductID')\n",
        "\n",
        "# Drop unnecessary columns ('CustomerName' and 'ProductName' not needed for analysis)\n",
        "merged_df.drop(columns=['CustomerName', 'ProductName'], inplace=True)\n",
        "\n",
        "# Check for missing values and handle them\n",
        "merged_df.fillna(0, inplace=True)\n"
      ],
      "metadata": {
        "id": "qFA0r7DShnTv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the data for analysis, we started by converting the SignupDate and TransactionDate columns into a proper datetime format, making it easier to work with dates and analyze trends. Then, we combined the data from the customers, transactions, and products tables into one comprehensive dataset by merging them using common identifiers like CustomerID and ProductID. After that, we removed columns like CustomerName and ProductName since they werenâ€™t needed for our analysis, keeping the dataset focused and manageable. Finally, we handled missing values by replacing them with zeros to ensure the data was complete and ready for further steps. This approach leaves us with a clean, well-organized dataset to work with"
      ],
      "metadata": {
        "id": "JLJSi0WwrE5x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lNSjI0wMpOd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 3: Feature Engineering\n",
        "# Aggregate features to create a customer profile\n",
        "customer_features = merged_df.groupby('CustomerID').agg({\n",
        "    'Quantity': 'sum',  # Total items purchased\n",
        "    'TotalValue': 'sum',  # Total spending\n",
        "    'TransactionID': 'count',  # Purchase frequency\n",
        "    'TransactionDate': lambda x: (pd.to_datetime('today') - x.max()).days  # Recency\n",
        "}).reset_index()\n",
        "\n",
        "# Rename columns for better readability\n",
        "customer_features.rename(columns={\n",
        "    'Quantity': 'TotalQuantity',\n",
        "    'TotalValue': 'TotalSpending',\n",
        "    'TransactionID': 'PurchaseFrequency',\n",
        "    'TransactionDate': 'Recency'\n",
        "}, inplace=True)\n",
        "\n",
        "# Merge with customer demographic details\n",
        "final_customer_data = customer_features.merge(customers_df[['CustomerID', 'Region']], on='CustomerID')\n"
      ],
      "metadata": {
        "id": "mtUcD-Cli9oH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a detailed customer profile by grouping data based on CustomerID and calculating key features like total items purchased (TotalQuantity), total spending (TotalSpending), number of transactions (PurchaseFrequency), and days since the last purchase (Recency). These features give insights into each customer's purchasing behavior. The columns were renamed for better clarity, and we merged this data with demographic details like Region to provide a complete view of each customer."
      ],
      "metadata": {
        "id": "m6xlojGJryi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X4t1jh_NrK2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 4: Data Preprocessing and Scaling\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Encode categorical variables (Region)\n",
        "encoder = LabelEncoder()\n",
        "final_customer_data['Region'] = encoder.fit_transform(final_customer_data['Region'])\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(final_customer_data.drop(columns=['CustomerID']))\n",
        "\n",
        "# Convert to DataFrame for easy reference\n",
        "scaled_df = pd.DataFrame(scaled_features, columns=final_customer_data.columns[1:])\n",
        "scaled_df['CustomerID'] = final_customer_data['CustomerID']\n"
      ],
      "metadata": {
        "id": "1W5W0px_keTO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the data for modeling, we started by encoding the Region column, a categorical variable, using LabelEncoder to convert it into numerical values. Next, we standardized all numerical features to bring them onto the same scale using StandardScaler, ensuring fair treatment across features during modeling. The standardized data was then converted back into a DataFrame for easy interpretation, with column names preserved for clarity. Finally, the CustomerID column was re-added to maintain reference to individual customers"
      ],
      "metadata": {
        "id": "WYS0US5qr7FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 5: Train KNN Model and Find Similar Customers\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# Fit the KNN model with cosine similarity\n",
        "knn_model = NearestNeighbors(n_neighbors=4, metric='cosine')\n",
        "knn_model.fit(scaled_features)\n",
        "\n",
        "# Find lookalikes for the first 20 customers (C0001 - C0020)\n",
        "customer_ids = final_customer_data['CustomerID'][:20].values\n",
        "lookalike_map = {}\n",
        "\n",
        "for idx, customer in enumerate(scaled_features[:20]):\n",
        "    distances, indices = knn_model.kneighbors([customer])\n",
        "    similar_customers = final_customer_data.iloc[indices[0][1:]]  # Exclude itself\n",
        "\n",
        "    lookalike_map[customer_ids[idx]] = list(zip(\n",
        "        similar_customers['CustomerID'].values, distances[0][1:].round(2)\n",
        "    ))\n"
      ],
      "metadata": {
        "id": "OvcELzvnlBYl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the K-Nearest Neighbors (KNN) algorithm to find similar customers based on their standardized features. The model was trained with NearestNeighbors, using cosine similarity as the metric to measure the closeness between customers. For demonstration, we identified similar customers for the first 20 customer profiles in the dataset. For each of these customers, the model calculated distances to their nearest neighbors, excluding the customer itself. The lookalike_map was created to store the IDs of similar customers along with their corresponding similarity scores, allowing us to analyze and recommend similar customer profiles effectively."
      ],
      "metadata": {
        "id": "eKXYUiGSsDU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model evaluation\n",
        "import numpy as np\n",
        "\n",
        "# Calculating the average similarity score for the top 3 lookalikes of each customer\n",
        "avg_similarity_scores = []\n",
        "\n",
        "for cust_id, lookalikes in lookalike_map.items():\n",
        "    scores = [score for _, score in lookalikes]\n",
        "    avg_similarity_scores.append(np.mean(scores))\n",
        "\n",
        "print(f'Average Similarity Score: {np.mean(avg_similarity_scores):.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTSbrRWmlYq7",
        "outputId": "4abe0fa7-701b-4e5b-e58e-da05914bb2e6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Similarity Score: 0.0632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The evaluation of the KNN model resulted in an Average Similarity Score of 0.0632, indicating that the recommended lookalikes for each customer are closely aligned based on their standardized feature vectors. This low similarity score suggests that the customers identified as similar have minimal differences, showcasing the effectiveness of the model in clustering similar profiles."
      ],
      "metadata": {
        "id": "4f3RrGXWsJAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the lookalike dictionary into a DataFrame and save to CSV\n",
        "lookalike_df = pd.DataFrame(list(lookalike_map.items()), columns=['CustomerID', 'Lookalikes'])\n",
        "lookalike_df.to_csv('Lookalike.csv', index=False)\n",
        "\n",
        "print(\"Lookalike recommendations saved successfully in Lookalike.csv\")\n"
      ],
      "metadata": {
        "id": "-4wMNkRymRzu",
        "outputId": "434e053b-32ed-4ade-9631-5c40efa6034d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lookalike recommendations saved successfully in Lookalike.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XJaI1GutmpS7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}